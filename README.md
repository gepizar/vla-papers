# vla-papers

| Title | Link | 
|-------|------|
| Yell At Your Robot Improving On-the-Fly from Language Corrections | [Paper](https://arxiv.org/pdf/2403.12910.pdf) |
| RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control | [Paper](https://proceedings.mlr.press/v229/zitkovich23a/zitkovich23a.pdf) |
| A Joint Modeling of Vision-Language-Action for Target-oriented Grasping in Clutter | [Paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10161041) |
| ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts | [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Lin_ADAPT_Vision-Language_Navigation_With_Modality-Aligned_Action_Prompts_CVPR_2022_paper.pdf) |
| RT-H: Action Hierarchies Using Language | [Paper](https://arxiv.org/pdf/2403.01823.pdf) |
| RT-1: Robotics Transformer for Real-World Control at Scale | [Paper](https://arxiv.org/pdf/2212.06817.pdf) |
| Gesture-Informed Robot Assistance via Foundation Models | [Paper](https://openreview.net/pdf?id=Ffn8Z4Q-zU) |
| Robots that ask for help: Uncertainty Alignment for Large Language Model Planners | [Paper](https://arxiv.org/pdf/2307.01928.pdf) |
| Language-Driven Representation Learning for Robotics | [Paper](https://arxiv.org/pdf/2302.12766.pdf?trk=public_post_comment-text) |
| PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem | [Paper](https://arxiv.org/pdf/2402.10450v1.pdf) |
| Scaling Instructable Agents Across Many Simulated Worlds | [Paper](https://storage.googleapis.com/deepmind-media/DeepMind.com/Blog/sima-generalist-ai-agent-for-3d-virtual-environments/Scaling%20Instructable%20Agents%20Across%20Many%20Simulated%20Worlds.pdf) |
| PaLM-E: An Embodied Multimodal Language Model | [Paper](https://arxiv.org/pdf/2303.03378.pdf?trk=public_post_comment-text) |
| PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs | [Paper](https://arxiv.org/pdf/2402.07872.pdf) |
| Vision-Language Models Provide Promptable Representations for Reinforcement Learning | [Paper](https://arxiv.org/pdf/2402.02651.pdf) |
| MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting | [Paper](https://arxiv.org/pdf/2403.03174v1.pdf) |
|  | [Paper]() |
| Interactive Language: Talking to Robots in Real Time | [Paper](https://arxiv.org/pdf/2210.06407.pdf) |
| Language Conditioned Imitation Learning over Unstructured Data | [Paper](https://arxiv.org/pdf/2005.07648.pdf) |
| Language-Conditioned Path Planning | [Paper](https://proceedings.mlr.press/v229/xie23b/xie23b.pdf) |
| RoboVQA: Multimodal Long-Horizon Reasoning for Robotics | [Paper](https://arxiv.org/pdf/2311.00899.pdf) |
| AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents | [Paper](https://arxiv.org/pdf/2401.12963.pdf) |
| Look Before You Leap: Unveiling the Power of
GPT-4V in Robotic Vision-Language Planning | [Paper](https://robot-vila.github.io/ViLa.pdf) |
| Interactive Task Planning with Language Models | [Paper](https://arxiv.org/pdf/2310.10645.pdf) |
| Video Language Planning | [Paper](https://arxiv.org/pdf/2310.10625.pdf) |
| Large Langueage Models are Visual Reasoning Coordinators | [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/ddfe6bae7b869e819f842753009b94ad-Paper-Conference.pdf) |
| Learning Universal Policies via Text-Guided Video Generation | [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf) |
| Video as the New Language for Real-World Decision Making | [Paper](https://arxiv.org/pdf/2402.17139.pdf) |

# Grounding / Affordance
| Title | Link | 
|-------|------|
| Do As I Can, Not As I Say: Grounding Language in Robotic Affordances | [Paper](https://proceedings.mlr.press/v205/ichter23a/ichter23a.pdf) |
| Physically Ground Vision-Language Models for Robotic Manipulation | [Paper](https://arxiv.org/pdf/2309.02561v4.pdf) |
| Grounded Decoding: Guiding Text Generation with Grounded Models for Embodied Agents | [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/bb3cfcb0284642a973dd631ec9184f2f-Paper-Conference.pdf) |
| LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models | [Paper](https://arxiv.org/pdf/2312.02949v1.pdf) |
|ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models| [Paper](https://arxiv.org/pdf/2403.11289.pdf) |




# Other
| Title | Link | 
|-------|------|
| Pushing the Limits of Cross-Embodiment Learning for Manipulation and Navigation | [Paper](https://arxiv.org/pdf/2402.19432v1.pdf) |
| Goal Representations for Instruction Following: A Semi-Supervised Language Interface to Control | [Paper](https://proceedings.mlr.press/v229/myers23a/myers23a.pdf) |
| Hierarchical Reinforcement Learning in
Complex 3D Environments | [Paper](https://arxiv.org/pdf/2302.14451.pdf) |
| Genie: Generative Interactive Environments | [Paper](https://arxiv.org/pdf/2402.15391v1.pdf) |
| Large Language Models Can Implement Policy Iteration | [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/60dc7fa827f5f761ad481e2ad40b5573-Paper-Conference.pdf) | 
| Efficient Data Collection for Robotic Manipulation via Compositional Generalization | [Paper](https://arxiv.org/pdf/2403.05110.pdf) |
| Robots That Can See: Leveraging Human Pose for Trajectory Prediction | [Paper](https://arxiv.org/pdf/2309.17209.pdf) |
| Mitigating Spurious Correlations in Multi-modal Models during Fine-tunning | [Paper](https://proceedings.mlr.press/v202/yang23j/yang23j.pdf) |
| Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning | [Paper](https://proceedings.mlr.press/v229/shah23c/shah23c.pdf) |
| Gradient-based Planning with World Models | [Paper](https://arxiv.org/pdf/2312.17227.pdf) |
| A Path Towards Autonomous Machine Intelligence | [Paper](https://openreview.net/pdf?id=BZ5a1r-kVsf) |

https://arxiv.org/abs/2403.17844

# MM LLM
| Title | Link |
|-------|------|
| MM1: Methods, Analysis & Insights from Multimodal LLM Pre-training | [Paper](https://arxiv.org/pdf/2403.09611.pdf) |
| MM-LLms. Recent Advances in MultiModal Large Language Models | [Paper](https://arxiv.org/pdf/2401.13601.pdf?trk=public_post_comment-text) |
| Gemini: A Family of Highly Capable Multimodal Models | [Paper](https://arxiv.org/pdf/2312.11805.pdf) |




# Datasets / Benchmark
| Title | Link |
|-------|------|
| Open X-Embodiment: Robotic Learning Datasets and RT-X Models | [Paper](https://openreview.net/pdf?id=zraBtFgxT0) |
| DROID: A Large-Scale In-The-Wild Robot Manipulation Dataset | [Paper](https://arxiv.org/pdf/2403.12945.pdf) | [Paper](https://arxiv.org/pdf/2403.12945.pdf) |
| DataCOMP: In search of the next generation of multimodal datasets | [Paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/56332d41d55ad7ad8024aac625881be7-Paper-Datasets_and_Benchmarks.pdf) |
| Multimodal Algorithmic Reasoning Workshop (CVPR2024). Challenge: vision-and-language reasoning on abstract visual puzzles. SMART-101 dataset | [1. Workshop](https://marworkshop.github.io/cvpr24/index.html) [2. Dataset](https://smartdataset.github.io/smart101/)|
| Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives | [Paper](https://ego-exo4d-data.org/paper/ego-exo4d.pdf) |




https://www.youtube.com/watch?v=akDSG9FsoCk&ab_channel=YunzhuLi


# Compositional Learning
| Title | Link |
| ----- | ----- |
| How to Grow a Mind: Statistics, Structure, and Abstraction | [Paper](https://www.science.org/doi/pdf/10.1126/science.1192788) |
| Human-level concept learning through probabilistic program induction | [Paper](https://www.science.org/doi/epdf/10.1126/science.aab3050) |
| Compositional generalization through meta sequence-to-sequence learning | [Paper](https://proceedings.neurips.cc/paper_files/paper/2019/file/f4d0e2e7fc057a58f7ca4a391f01940a-Paper.pdf) |
| Human-link systematic generalization through a meta-learning neural network | [Paper](https://www.nature.com/articles/s41586-023-06668-3) |


